{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Chat Model can be built using <b><u>BaseChatModel interface</u></b>\n",
    "#### We will need to implement the following properties or methods.\n",
    "* _generate (method) (required) : Use to generate a chat result from a prompt\n",
    "* _llm_type (property) (required) : Used to uniquely identify the type of the model. <b>Used for logging</b>\n",
    "* _stream, _agenerate, _astream (method) (optional) \n",
    "\n",
    "#### A custom chat model that echoes the first `n` characters of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterator, List, Optional, Dict\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessageChunk, AIMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult, ChatGenerationChunk\n",
    "\n",
    "\"\"\"\n",
    "_generate method returns ChatResult\n",
    "ChatResult is a list of ChatGeneration\n",
    "ChatGeneration returns the message(AIMessage)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class CustomModel(BaseChatModel):\n",
    "\n",
    "    \"\"\" A custom chat model that echoes the first `n` characters of the input. \"\"\"\n",
    "    \n",
    "    model_name: str\n",
    "    \"\"\" Name of the model \"\"\"\n",
    "\n",
    "    n : int\n",
    "    \"\"\" The number of characters from the last message of the prompt to be echoed. \"\"\"\n",
    "\n",
    "    def _generate(self, \n",
    "                  messages: List[BaseMessage], \n",
    "                  stop: Optional[List[str]] = None, \n",
    "                  run_manager: Optional[CallbackManagerForLLMRun | None] = None, \n",
    "                  **kwargs: Any) -> ChatResult:\n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM. \"\"\"\n",
    "        \n",
    "        \"\"\" This is the default return statement but we need to update it to return n characters from message\"\"\"\n",
    "        #return super()._generate(messages, stop, run_manager, **kwargs)\n",
    "\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[:self.n]\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
    "            response_metadata={  # Use for response metadata\n",
    "                \"time_in_seconds\": 3,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"echoing-chat-model\"\n",
    "    \n",
    "    def _stream(self, \n",
    "                messages: List[BaseMessage], \n",
    "                stop: Optional[List[str]] | None = None, \n",
    "                run_manager: Optional[CallbackManagerForLLMRun] | None = None, \n",
    "                **kwargs: Any) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\" This is the default return statement but we need to update it to return n characters from message\"\"\"\n",
    "        # return super()._stream(messages, stop, run_manager, **kwargs)\n",
    "\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[:self.n]\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n",
    "\n",
    "            if run_manager:\n",
    "                # This is optional in newer versions of LangChain\n",
    "                # The on_llm_new_token will be called automatically\n",
    "                run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "\n",
    "            yield chunk\n",
    "\n",
    "        # Let's add some other information (e.g., response metadata)\n",
    "        chunk = ChatGenerationChunk(\n",
    "            message=AIMessageChunk(content=\"\", response_metadata={\"time_in_sec\": 3})\n",
    "        )\n",
    "        if run_manager:\n",
    "            # This is optional in newer versions of LangChain\n",
    "            # The on_llm_new_token will be called automatically\n",
    "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "        yield chunk\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": self.model_name,\n",
    "        }        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meo', response_metadata={'time_in_seconds': 3}, id='run-ed236f67-ba09-472a-8488-8ce7759400f0-0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = CustomModel(n=3,model_name=\"test\")\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hello!\"),\n",
    "        AIMessage(content=\"Hi there human!\"),\n",
    "        HumanMessage(content=\"Meow!\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# llm.generate(\"Hello\")\n",
    "# llm.invoke([(\"human\",\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H|e|l||"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Hello\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvLangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
