{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='age: 55.0\\nanaemia: 0\\ncreatinine_phosphokinase: 748\\ndiabetes: 0\\nejection_fraction: 45\\nhigh_blood_pressure: 0\\nplatelets: 263358.03\\nserum_creatinine: 1.3\\nserum_sodium: 137\\nsex: 1\\nsmoking: 1\\ntime: 88\\nDEATH_EVENT: 0', metadata={'source': './heart_failure_clinical_records.csv', 'row': 0}),\n",
       " Document(page_content='age: 65.0\\nanaemia: 0\\ncreatinine_phosphokinase: 56\\ndiabetes: 0\\nejection_fraction: 25\\nhigh_blood_pressure: 0\\nplatelets: 305000.0\\nserum_creatinine: 5.0\\nserum_sodium: 130\\nsex: 1\\nsmoking: 0\\ntime: 207\\nDEATH_EVENT: 0', metadata={'source': './heart_failure_clinical_records.csv', 'row': 1}),\n",
       " Document(page_content='age: 45.0\\nanaemia: 0\\ncreatinine_phosphokinase: 582\\ndiabetes: 1\\nejection_fraction: 38\\nhigh_blood_pressure: 0\\nplatelets: 319000.0\\nserum_creatinine: 0.9\\nserum_sodium: 140\\nsex: 0\\nsmoking: 0\\ntime: 244\\nDEATH_EVENT: 0', metadata={'source': './heart_failure_clinical_records.csv', 'row': 2}),\n",
       " Document(page_content='age: 60.0\\nanaemia: 1\\ncreatinine_phosphokinase: 754\\ndiabetes: 1\\nejection_fraction: 40\\nhigh_blood_pressure: 1\\nplatelets: 328000.0\\nserum_creatinine: 1.2\\nserum_sodium: 126\\nsex: 1\\nsmoking: 0\\ntime: 90\\nDEATH_EVENT: 0', metadata={'source': './heart_failure_clinical_records.csv', 'row': 3}),\n",
       " Document(page_content='age: 95.0\\nanaemia: 1\\ncreatinine_phosphokinase: 582\\ndiabetes: 0\\nejection_fraction: 30\\nhigh_blood_pressure: 0\\nplatelets: 461000.0\\nserum_creatinine: 2.0\\nserum_sodium: 132\\nsex: 1\\nsmoking: 0\\ntime: 50\\nDEATH_EVENT: 1', metadata={'source': './heart_failure_clinical_records.csv', 'row': 4})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(\"./heart_failure_clinical_records.csv\")\n",
    "\n",
    "loader.load()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify a column to identify the document source\n",
    "\n",
    "Use the source_column argument to specify a source for the document created from each row. Otherwise file_path will be used as the source for all documents created from the CSV file.\n",
    "\n",
    "<u><i>This is useful when using documents loaded from CSV files for chains that answer questions using sources.</i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='age: 55.0\\nanaemia: 0\\ncreatinine_phosphokinase: 748\\ndiabetes: 0\\nejection_fraction: 45\\nhigh_blood_pressure: 0\\nplatelets: 263358.03\\nserum_creatinine: 1.3\\nserum_sodium: 137\\nsex: 1\\nsmoking: 1\\ntime: 88\\nDEATH_EVENT: 0', metadata={'source': '0', 'row': 0}),\n",
       " Document(page_content='age: 65.0\\nanaemia: 0\\ncreatinine_phosphokinase: 56\\ndiabetes: 0\\nejection_fraction: 25\\nhigh_blood_pressure: 0\\nplatelets: 305000.0\\nserum_creatinine: 5.0\\nserum_sodium: 130\\nsex: 1\\nsmoking: 0\\ntime: 207\\nDEATH_EVENT: 0', metadata={'source': '0', 'row': 1}),\n",
       " Document(page_content='age: 45.0\\nanaemia: 0\\ncreatinine_phosphokinase: 582\\ndiabetes: 1\\nejection_fraction: 38\\nhigh_blood_pressure: 0\\nplatelets: 319000.0\\nserum_creatinine: 0.9\\nserum_sodium: 140\\nsex: 0\\nsmoking: 0\\ntime: 244\\nDEATH_EVENT: 0', metadata={'source': '1', 'row': 2}),\n",
       " Document(page_content='age: 60.0\\nanaemia: 1\\ncreatinine_phosphokinase: 754\\ndiabetes: 1\\nejection_fraction: 40\\nhigh_blood_pressure: 1\\nplatelets: 328000.0\\nserum_creatinine: 1.2\\nserum_sodium: 126\\nsex: 1\\nsmoking: 0\\ntime: 90\\nDEATH_EVENT: 0', metadata={'source': '1', 'row': 3}),\n",
       " Document(page_content='age: 95.0\\nanaemia: 1\\ncreatinine_phosphokinase: 582\\ndiabetes: 0\\nejection_fraction: 30\\nhigh_blood_pressure: 0\\nplatelets: 461000.0\\nserum_creatinine: 2.0\\nserum_sodium: 132\\nsex: 1\\nsmoking: 0\\ntime: 50\\nDEATH_EVENT: 1', metadata={'source': '0', 'row': 4})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"./heart_failure_clinical_records.csv\", csv_args={\"delimiter\":\",\",\"quotechar\":'\"'},source_column=\"diabetes\")\n",
    "\n",
    "loader.load()[:5]\n",
    "\n",
    "#print(loader.source_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Directory Loader : <i>Load all documents in a directory</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "import unstr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file heart_failure_clinical_records.csv\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "partition_csv is not available. Install the csv dependencies with pip install \"unstructured[csv]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:181\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m    184\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:219\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/langchain_community/document_loaders/directory.py:209\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    207\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_cls(\u001b[38;5;28mstr\u001b[39m(item), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_kwargs)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubdoc\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/langchain_community/document_loaders/unstructured.py:88\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/langchain_community/document_loaders/unstructured.py:180\u001b[0m, in \u001b[0;36mUnstructuredFileLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, Path):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/unstructured/partition/auto.py:496\u001b[0m, in \u001b[0;36mpartition\u001b[0;34m(filename, content_type, file, file_filename, url, include_page_breaks, strategy, encoding, paragraph_grouper, headers, skip_infer_table_types, ssl_verify, ocr_languages, languages, detect_language_per_element, pdf_infer_table_structure, pdf_extract_images, pdf_extract_element_types, pdf_image_output_dir_path, pdf_extract_to_payload, xml_keep_tags, data_source_metadata, metadata_filename, request_timeout, hi_res_model_name, model_name, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     elements \u001b[38;5;241m=\u001b[39m _partition_xlsx(\n\u001b[1;32m    488\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    489\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mCSV:\n\u001b[0;32m--> 496\u001b[0m     _partition_csv \u001b[38;5;241m=\u001b[39m \u001b[43m_get_partition_with_extras\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     elements \u001b[38;5;241m=\u001b[39m _partition_csv(\n\u001b[1;32m    498\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m    499\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    504\u001b[0m     )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filetype \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mTSV:\n",
      "File \u001b[0;32m~/Documents/GenAI/Langchain/venvLangchain/lib/python3.12/site-packages/unstructured/partition/auto.py:114\u001b[0m, in \u001b[0;36m_get_partition_with_extras\u001b[0;34m(doc_type, partition_with_extras_map)\u001b[0m\n\u001b[1;32m    112\u001b[0m _partition_func \u001b[38;5;241m=\u001b[39m partition_with_extras_map\u001b[38;5;241m.\u001b[39mget(doc_type)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _partition_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartition_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dependencies with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munstructured[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _partition_func\n",
      "\u001b[0;31mImportError\u001b[0m: partition_csv is not available. Install the csv dependencies with pip install \"unstructured[csv]\""
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\".\",glob=\"*.csv\")\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"2404.19553v1.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Extending Llama-3’s Context Ten-Fold Overnight\\nPeitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\\nQiwei Ye1, Zhicheng Dou2\\n1Beijing Academy of Artificial Intelligence\\n2Gaoling School of Artificial Intelligence, Renmin University of China\\nnamespace.pt@gmail.com zhengliu1026@gmail.com\\nAbstract\\nWe extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\\nfine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\\n8xA800 (80G) GPU machine. The resulted model exhibits superior performances\\nacross a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\\ncontext language understanding; meanwhile, it also well preserves the original\\ncapability over short contexts. The dramatic context extension is mainly attributed\\nto merely 3.5K synthetic training samples generated by GPT-4 , which indicates\\nthe LLMs’ inherent (yet largely underestimated) potential to extend its original\\ncontext length. In fact, the context length could be extended far beyond 80K\\nwith more computation resources. Therefore, the team will publicly release the\\nentire resources (including data, model, data generation pipeline, training code) so\\nas to facilitate the future research from the community: https://github.com/\\nFlagOpen/FlagEmbedding .\\n1 Introduction\\nRecently, considerable attention has been directed towards long-context large language models,\\nwhere different approaches are adopted to establish long-context capabilities for large language\\nmodels [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\\naccomplish.\\nIn this technical report, we propose an efficient solution for entitling the long-context capabilities for\\nLLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\\nwe use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\\n1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\\ndata for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\\nfrom a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\\nquestion-answer pairs based on this segment.\\n2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\\ndetails in a long context. We define two types of long context. The homogeneous\\ncontext contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\\ngenerate multiple question-answer pairs that require aggregating and analyzing information\\nfrom different locations in the context. The heterogeneous context consists of multiple\\nindependent texts. Notably, we perform clustering over a large corpus then extract texts from\\n∗Corresponding author.\\n2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\\nHowever, users could apply the model for even longer contexts via extrapolation.\\n3https://llama.meta.com/llama3/\\nPreprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024', metadata={'source': '2404.19553v1.pdf', 'page': 0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 800014315 20631 26947 33263 39578 45894 52210 58526 64842 71157 77473 83789 90105 96421102736 109052 115368 121684 128000\n",
      "Context Length0\n",
      "11\n",
      "22\n",
      "33\n",
      "44\n",
      "55\n",
      "66\n",
      "77\n",
      "88\n",
      "100Depth Percent1.0Needle In A HayStack\n",
      "12345678910\n",
      "Accuracy Score from GPT3.5Figure 1: The accuracy score of Llama-3-8B-Instruct-80K-QLoRA on Needle-In-A-HayStack task.\n",
      "The blue vertical line indicates the training length, i.e. 80K.\n",
      "the same cluster to form each heterogeneous context. Therefore, the grouped texts share\n",
      "some semantic similarity. We then prompt GPT-4 to ask about the similarities/dissimilarities\n",
      "across these texts.\n",
      "3.Biography Summarization : we prompt GPT-4 to write a biography for each main character\n",
      "in a given book.\n",
      "For all three tasks, the length of context is between 64K to 80K. Note that longer data can also be\n",
      "synthesized following the same methodology. When training, we organize the question-answer pairs\n",
      "for the same context in one multi-turn conversation then fine-tune the LLM to correctly answer the\n",
      "questions given the entire long context as input. Following previous work4, we mix 5K instances\n",
      "randomly chosen from RedPajama [ 6] to mitigate forgetting. We also mix LongAlpaca [ 5] in the\n",
      "training set, which contains 12K instruction tuning instances with 16K length at maximum. Therefore,\n",
      "the entire training dataset contains 20K instances.\n",
      "We use QLoRA [ 7] to efficiently fine-tune the model. We apply LoRA on all Q,K,V ,O projections\n",
      "and additionally train the embedding layer. We set LoRA rank to 32 and alpha to 16. The learning\n",
      "rate is 5e-5 with linear decay and no warmups. The batch size is 8. Gradient checkpointing is enabled.\n",
      "No parallel strategy is required thanks to the efficient implementation from Unsloth [ 1]. We train the\n",
      "model for 1 epoch, which takes 8 hours to complete on a 8xA800 (80G) machine. Importantly, we\n",
      "expand the RoPE base from 500K to 200M in training.\n",
      "Our contributions are highlighted as follows:\n",
      "•We release Llama-3-8B-Instruct-80K-QLoRA, which extends the context length of Llama-\n",
      "3-8B-Instruct from 8K to 80K. The entire resources including the model, training data, and\n",
      "code are all publicly available, which may advance the field of training long-context LLMs.\n",
      "•Our training recipe is simple and efficient, while the resulted model demonstrates remark-\n",
      "able performance on downstream long-context tasks. Further research can be made to\n",
      "improve our approach.\n",
      "2 Experiments\n",
      "We evaluate our model on popular long-context benchmarks, then compare it with the original\n",
      "Llama-3-8B-Instruct model and the long-context Llama-3-8B-Instruct-262K from the community5.\n",
      "4https://www.together.ai/blog/llama-2-7b-32k\n",
      "5https://huggingface.co/gradientai/Llama-3-8B-Instruct-262k\n",
      "2\n",
      "0: Extending Llama-3’s Context Ten-Fold Overnight\n",
      "Peitian Zhang1,2, Ninglu Shao1,2, Zheng Liu1∗, Shitao Xiao1, Hongjin Qian1,2,\n",
      "Qiwei Ye1, Zhicheng Dou2\n",
      "1Beijing Academy of Artificial Intelligence\n",
      "2Gaoling School of Artificial Intelligence, Renmin University of China\n",
      "namespace.pt@gmail.com zhengliu1026@gmail.com\n",
      "Abstract\n",
      "We extend the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA\n",
      "fine-tuning2. The entire training cycle is super efficient, which takes 8 hours on one\n",
      "8xA800 (80G) GPU machine. The resulted model exhibits superior performances\n",
      "across a broad range of evaluation tasks, such as NIHS, topic retrieval, and long-\n",
      "context language understanding; meanwhile, it also well preserves the original\n",
      "capability over short contexts. The dramatic context extension is mainly attributed\n",
      "to merely 3.5K synthetic training samples generated by GPT-4 , which indicates\n",
      "the LLMs’ inherent (yet largely underestimated) potential to extend its original\n",
      "context length. In fact, the context length could be extended far beyond 80K\n",
      "with more computation resources. Therefore, the team will publicly release the\n",
      "entire resources (including data, model, data generation pipeline, training code) so\n",
      "as to facilitate the future research from the community: https://github.com/\n",
      "FlagOpen/FlagEmbedding .\n",
      "1 Introduction\n",
      "Recently, considerable attention has been directed towards long-context large language models,\n",
      "where different approaches are adopted to establish long-context capabilities for large language\n",
      "models [ 4,14,5,8,9,16,2]. However, most of them require significant compute and resources to\n",
      "accomplish.\n",
      "In this technical report, we propose an efficient solution for entitling the long-context capabilities for\n",
      "LLMs, with which we extend the context length of Llama-3-8B-Instruct3from 8K to 80K. Specifically,\n",
      "we use GPT-4 [13] to synthesize 3.5K long-context training data, covering three long-context tasks:\n",
      "1.Single-Detail QA : the inquiry targets on one specific detail in a long context. To construct\n",
      "data for this task, we slice out a short segment (e.g., a chunk with less than 4096 tokens)\n",
      "from a long context (e.g., a book or a long paper) and prompt GPT-4 to generate multiple\n",
      "question-answer pairs based on this segment.\n",
      "2.Multi-Detail QA : the inquiry requires information aggregation and reasoning over multiple\n",
      "details in a long context. We define two types of long context. The homogeneous\n",
      "context contains a coherent text, such as a book or a long paper. We prompt GPT-4 to\n",
      "generate multiple question-answer pairs that require aggregating and analyzing information\n",
      "from different locations in the context. The heterogeneous context consists of multiple\n",
      "independent texts. Notably, we perform clustering over a large corpus then extract texts from\n",
      "∗Corresponding author.\n",
      "2The model is noted as Llama-3-8B-Instruct-80K-QLoRA given its max context length during fine-tuning.\n",
      "However, users could apply the model for even longer contexts via extrapolation.\n",
      "3https://llama.meta.com/llama3/\n",
      "Preprint. Under review.arXiv:2404.19553v1  [cs.CL]  30 Apr 2024\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pprint import pprint\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vector_store = FAISS.from_documents(documents=pages,embedding=embeddings)\n",
    "\n",
    "docs= vector_store.similarity_search(\"What was the increased context length\",k=2)\n",
    "\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) +\":\",doc.page_content )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a JSON File\n",
    "import requests \n",
    "from pprint import pprint \n",
    "import json\n",
    "\n",
    "#Download JSON from URL\n",
    "url = \"https://filesamples.com/samples/code/json/sample4.json\"\n",
    "json_data = requests.get(url).json()\n",
    "\n",
    "#Create JSON Object from JSON Response Data\n",
    "json_object= json.dumps(json_data)\n",
    "\n",
    "#Write JSON Object into JSON File\n",
    "file = \"JsonData.json\"\n",
    "with open(file, \"w\") as json_file:\n",
    "    json_file.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvLangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
